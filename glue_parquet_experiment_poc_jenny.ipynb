{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an end-to-end conversion: from the original CSVs in S3, to Pandas dataframe and then parquet locally, then uploaded into an s3 test bucket and catalogued in Glue.\n",
    "\n",
    "Experiment:\n",
    "\n",
    "1. Take a full day of Adobe files\n",
    "2. Import them to a Pandas DataFrame with all types cast to str\n",
    "3. Export them to parquet\n",
    "4. Copy them to S3\n",
    "5. Run the Glue crawler\n",
    "6. Set all columns to string (experiment iteration 2 only)\n",
    "7. Attempt to query with Athena\n",
    "\n",
    "Something about the data:\n",
    "\n",
    "The data used is Adobe hit. The data is put in `jp-insights-dst-adobe-sydney` bucket and partitioned in group/year/month/day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your own POC credentials\n",
    "\n",
    "myAccessKey = 'your_aws_access_key'\n",
    "mySecretKey = 'your_aws_secret_key'\n",
    "\n",
    "\n",
    "# Start session with POC\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=myAccessKey,\n",
    "    aws_secret_access_key=mySecretKey)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "glue = boto3.client('glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets():\n",
    "    return[bucket.name for bucket in s3.buckets.all()]\n",
    "\n",
    "get_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from bucket\n",
    "csv_bucket_name = 'jp-insights-dst-adobe-sydney'\n",
    "prefix_path = 'group/2019/01/01'\n",
    "bucket = s3.Bucket(csv_bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_files = []\n",
    "for x in bucket.objects.filter(Prefix=prefix_path):\n",
    "    if 'hit_data' in x.key: \n",
    "        target_files.append(x.key)\n",
    "        \n",
    "# or you can run this code, it's shorter        \n",
    "#target_files = [x.key for x in bucket.objects.filter(Prefix=prefix_path) if 'hit_data' in x.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_bucket_name = 'jp-insights-dst-adobe-sydney'\n",
    "prefix_path = 'group/2019/01/01'\n",
    "bucket = s3.Bucket(csv_bucket_name)\n",
    "target_files = [x.key for x in bucket.objects.filter(Prefix=prefix_path) if 'hit_data' in x.key]\n",
    "\n",
    "for f in target_files:\n",
    "    names = re.match(prefix_path + '/.*?/v1/(.*)', f)\n",
    "    file_name = names.group(1)\n",
    "    s3.meta.client.download_file(csv_bucket_name, f, './%s' % file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each downloaded CSV into a DF as all str, then write out to parquet\n",
    "\n",
    "headers = ['col%d' % x for x in range(0, 1006)]\n",
    "\n",
    "for f in target_files:\n",
    "    names = re.match(prefix_path + '/.*?/v1/(.*)', f)\n",
    "    file_name = names.group(1)\n",
    "    df = pd.read_csv('./' + file_name, names=headers, dtype=str)\n",
    "    print(df.head())\n",
    "    df.to_parquet('./' + file_name.replace('.csv.gz', '.parquet'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the dataframe in CSV\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine parquet file to one folder\n",
    "\n",
    "parquet_files = [x for x in os.listdir('./') if 'parquet' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the parquet file\n",
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get them into our s3 bucket for Glue\n",
    "\n",
    "for f in parquet_files:\n",
    "    print(f)\n",
    "    s3.meta.client.upload_file('./' + f, 'your_parquet_file_in_S3', 'hit_data/' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crawler to create the table\n",
    "glue.start_crawler(Name='Your_crawler_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
